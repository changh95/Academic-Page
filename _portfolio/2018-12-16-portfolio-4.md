---
title: "Survivor Search and Environment Mapping Rover for Disaster Situations"
excerpt: "Built a rover system using NVidia Jetson TX2 and STM32, that is capable of mapping disaster environment and simultaneously searching for survivors."
collection: portfolio
---

This project was taken as a part of [AI Plaython - Embedded AI Implementation Hackathon](https://event-us.kr/modu/event/4448), organised by [MODU Labs](http://www.modulabs.co.kr/). In this project, I worked with a very enthusiastic and extremely creative engineer -  Dong-Won Shin, who is currently undertaking a Ph.D programme at GIST. We worked really hard together, and our effort has led us to winning the hackathon competition. It was a great pleasure to work with him! :) 

![alt_text](https://github.com/changh95/changh95.github.io/blob/master/images/portfolio_4_4.jpg?raw=true)

The aim of the hackathon was to be creative in using NVidia Jetson TX2 board, which is basically an embedded board system that can do various communicatins (e.g. serial, i2c, U-ART) and has a relatively good CPU & GPU.

---------------------------------------------------

Dong-Won and I decided to explore the potential of using Jetson TX2 in a **Search and Rescue Robot for Earthquake Situations**. We analysed the needs in the search and rescue operations in earthquake situations, and developed a robot that fits the needs. Our presentation file is available [Here](https://github.com/changh95/changh95.github.io/blob/master/files/20181215%20AI%20%ED%94%8C%EB%A0%88%EC%9D%B4%ED%86%A4%20%EB%B0%9C%ED%91%9C%EC%9E%90%EB%A3%8C.pptx?raw=true)

![alt_text](https://github.com/changh95/changh95.github.io/blob/master/files/portfolio-4-5.jpg?raw=true)

In summary, this robot is capable of:
  - A **'Rover'** function (i.e. a continuous track) that is capable of travelling on uneven surfaces.
  - A **'Mapping'** function, to make sure we do not re-visit the location the robot travelled, so that we don't waste time in searching for survivors.
  - A **'Collision Prevention'** function, to reduce the chances of getting the robot damaged, which we can expect reducing potential downtimes, hence higher chance of finding survivors.
  - An **'Automatic Survivor Detection'** function, to allow fast detection of survivors, as well as avoiding robot operator's human errors in locating survivors in the robot's field of view.
  
--------------------------------------------------
  
The 'Rover' function is developed by using STM32 module to control a continuous track actuator. To allow for a remote control, we established this communication chain: 

 ![alt_text](https://github.com/changh95/changh95.github.io/blob/master/files/portfolio-4-1.png?raw=true)

The 'Mapping' function is done by using a SLAM algorithm called 'RTAB-MAP', using Kinect camera. On ROS environment, we could extract 3D point cloud and visualise it on MeshLab.

![alt_text](https://github.com/changh95/changh95.github.io/blob/master/files/portfolio-4-2.png?raw=true)

The 'Collision Prevention' function is done by using RealSense D435 camera. We could convert the forward field of view into 3D pointcloud using infrared depth estimation, and then created an if statement to automatically stop the rover when 30% of field of view has data loss. The data loss occurs when an object is too close to the camera, which is generally considered as a limitation of infrared depth estimation method, but we used this property as a condition to notify the robot operator that an object or a wall is too close to the robot.

![alt_text](https://github.com/changh95/changh95.github.io/blob/master/files/portfolio-4-3.png?raw=true)

The 'Automatic Survivor Detection' function is done by using Tiny-YOLO object detection library. Although possible, we did not fine-tune the network due to the time limit for this hackathon, but the original network already had a not-too-bad performance in human detection. When a human is spotted in the field of view, the robot sends the map data to the operator, and at the same time, estimates where the human is relative to the camera frame. With the trajectory data obtained from SLAM algorithm, we can find the location of human relative to the robot's starting position, therefore allowing the rescue team to reach the survivor in the shortest distance.

![alt_text](https://github.com/changh95/changh95.github.io/blob/master/files/portfolio-4-4.png?raw=true)

In terms of the data processing time, all the algorithms could not run simultaneously at one Jetson TX2 board. Therefore, we used 2 Jetson TX2 board - one for rover control and SLAM, another for collision prevention and survivor detection.

-------------------------------

Just some pictures of me and Dong-Won working together!

![alt_text](https://github.com/changh95/changh95.github.io/blob/master/images/portfolio_4_1.jpg?raw=true)
![alt_text](https://github.com/changh95/changh95.github.io/blob/master/images/portfolio_4_3.jpg?raw=true)
![alt_text](https://github.com/changh95/changh95.github.io/blob/master/files/portfolio-4-gif.gif?raw=true)
